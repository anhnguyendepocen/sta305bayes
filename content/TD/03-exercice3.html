---
title: "Exercice 3"
linktitle: "Exercice 3"
date: "2020-11-30"
exo_date: "2020-12-04"
menu:
  td:
    parent: "TD"
    weight: 3
type: docs
output:
  blogdown::html_page:
    toc: false
    number_sections: false
bibliography: "../../static/bib/references.bib"
---



<p>On observe les réalisations <span class="math inline">\(\boldsymbol{x} = \{x_1,\ldots,x_n\}\)</span> d’une suite de variables aléatoires <span class="math inline">\(iid\)</span> <span class="math inline">\(\{X_i\}_{i=1, \dots, n}\)</span> réelles et supérieures à 1, dont la loi <span class="math inline">\(P_\theta\)</span> est supposée connue à un paramètre <span class="math inline">\(\theta &gt; 0\)</span> près. Cette loi <span class="math inline">\(P_\theta\)</span> est une loi continue, appelée loi de Pareto de paramètres <span class="math inline">\((\theta+1,1)\)</span> dont la densité est définie, pour <span class="math inline">\(x&gt;1\)</span>, par :
<span class="math display">\[\begin{align*}
f_\theta(x) &amp;=\frac{\theta+1}{x^{\theta+2}}
\end{align*}\]</span></p>
<ol style="list-style-type: decimal">
<li>La loi <em>a priori</em> utilisée pour <span class="math inline">\(\theta\)</span> est une loi exponentielle de paramètre 1, dont la fonction de densité s’écrit :
<span class="math inline">\(g(\theta) = e^{-\theta}\)</span>. Écrire alors le modèle bayésien associé.</li>
</ol>
<div class="Correction">
<ol style="list-style-type: decimal">
<li><p><strong>Question d’intérêt</strong><br />
Elle porte ici sur l’estimation de <span class="math inline">\(\theta\)</span>, le paramètre de la loi de Pareto supposée pour les observations.</p></li>
<li><p><strong>Modèle d’échantillonnage</strong>
<span class="math display">\[X_i | \theta \overset{iid}{\sim} \text{Pareto}(\theta +1, 1)\]</span>
et donc <span class="math inline">\(\displaystyle f(x_i|\theta) =\frac{\theta+1}{x_i^{\theta+2}}\)</span></p></li>
<li><p><strong>loi(s) <em>a priori</em></strong>
<span class="math display">\[\theta \sim \mathcal{E}(1)\]</span></p></li>
</ol>
</div>
<p><br></p>
<ol start="2" style="list-style-type: decimal">
<li>Montrer que la densité de la loi <em>a posteriori</em> de <span class="math inline">\(\theta | \boldsymbol{x}\)</span> — notée <span class="math inline">\(p(\theta|\boldsymbol{x})\)</span> — est proportionnelle à :</li>
</ol>
<p><span class="math display">\[e^{-\theta}\left(\theta+1\right)^{n}\prod_{i=1}^{n}x_i^{-\theta} \quad ; \quad \theta &gt; 0\]</span></p>
<div class="Correction">
<p>D’après le théorème de Bayes, on peut écrire que le numérateur de la loi <em>a posteriori</em> est proportionnel au produit de la densité de la loi <em>a priori</em> avec et la vraisemblance des données :</p>
<p><span class="math display">\[\begin{align*}
p(\theta | \boldsymbol{x}) &amp;\quad \propto  \quad \pi(\theta)f(x_1, ..., x_n | \theta) \\
                                        &amp;\quad \propto \quad e^{-\theta}\prod_{i=1}^n f(x_i| \theta) \\
                                        &amp;\quad \propto  \quad e^{-\theta} (\theta+1)^n \prod_{i=1}^n \frac{1}{x_i^{\theta+2}}\\
                                        &amp;\quad \propto  \quad e^{-\theta} (\theta+1)^n \left(\prod_{i=1}^n x_i^{-\theta}\right)  \left(\prod_{i=1}^n x_i^{-2}\right)
\end{align*}\]</span></p>
<p>Or <span class="math inline">\((\prod_{i=1}^n x_i^{-2})\)</span> est indépendant de <span class="math inline">\(\theta\)</span>, donc :
<span class="math display">\[\begin{align*}
p(\theta | \boldsymbol{x})\quad  \propto  \quad  e^{-\theta} (\theta+1)^n \prod_{i=1}^n x_i^{-\theta}  \qquad \theta \geq 0
\end{align*}\]</span></p>
</div>
<p><br></p>
<ol start="3" style="list-style-type: decimal">
<li>Proposer un algorithme de Metropolis-Hastings pour estimer la loi <em>a posteriori</em> de <span class="math inline">\(\theta | X_1,...,X_n\)</span>. On prendra comme loi instrumentale la loi <em>a priori</em> de <span class="math inline">\(\theta\)</span> (il s’agira donc d’un algorithme de Metropolis-Hastings indépendant). Expliciter l’estimateur Bayésien de <span class="math inline">\(\theta\)</span> construit pour le coût quadratique. Ne pas oublier de faire apparaître les calculs et la formule de la probabilité d’acceptation.</li>
</ol>
<div class="Correction">
<div class="Algo">
<ol style="list-style-type: decimal">
<li>Initialiser <span class="math inline">\(\theta^{(0)}\)</span></li>
<li>Pour <span class="math inline">\(t=1, \dots, n+N\)</span> faire :
<ol style="list-style-type: lower-alpha">
<li>Générer aléatoirement <span class="math inline">\(y^{(t)}\)</span> à partir de la loi <span class="math inline">\(\mathcal{E}(1)\)</span></li>
<li>Calcul de la probabilité d’acceptation <span class="math inline">\(\alpha^{(t)}\)</span> (voir après)</li>
<li>Étape d’acceptation-rejet de <span class="math inline">\(y^{(t)}\)</span>
<ul>
<li>Générer aléatoirement <span class="math inline">\(u^{(t)}\)</span> à partir de la loi <span class="math inline">\(U[0;1]\)</span></li>
<li><span class="math inline">\(\theta^{(t)} := \left\{ \begin{array}{l} y^{(t)} \text{ si } u^{(t)} \leq \alpha^{(t)} \\ \theta^{(t-1)} \text{ sinon} \end{array} \right.\)</span></li>
</ul></li>
</ol></li>
</ol>
</div>
<p>On détaille maintenant le calcul de la probabilité d’acceptation :
<span class="math display">\[\alpha^{(t)} = min \left( 1, \frac{p(y^{(t)} | \boldsymbol{x})}{p(\theta^{(t-1)} | \boldsymbol{x})}\frac{\pi(\theta^{(t-1)})}{\pi(y^{(t)})} \right)\]</span></p>
<p><span class="math display">\[\begin{align*}
\require{cancel}
          \dfrac{p(y^{(t)} | \boldsymbol{x})}{p(\theta^{(t-1)} | \boldsymbol{x})}\dfrac{\pi(\theta^{(t-1)})}{\pi(y^{(t)})} &amp; = \dfrac{p(y^{(t)} | \boldsymbol{x})}{p(\theta^{(t-1)} | \boldsymbol{x})}\dfrac{e^{-\theta^{(t-1)}}}{e^{-y^{(t)}}} \\
          &amp;  = \dfrac{(y^{(t)} +1)^n(\prod_{i=1}^n x_i^{-y^{(t)}})\cancel{e^{-y^{(t)}}}}{(\theta^{(t-1)}+1)^n\left(\prod_{k=1}^n x_i^{-\theta^{(t-1)} }\right)\cancel{e^{-\theta^{(t-1)}}}}\dfrac{\cancel{e^{-\theta^{(t-1)}}}}{\cancel{e^{-y^{(t)}}}}
\end{align*}\]</span></p>
<p>Finalement <span class="math inline">\(\alpha = min \left( 1, \dfrac{(y +1)^n \prod_{i=1}^n x_i^{-y}}{(\theta^{(t-1)} +1)^n \prod_{k=1}^n x_i^{-\theta^{(t-1)} }} \right)\)</span></p>
<p>Cet algorithme échantillonne alors selon la loi <em>a posteriori</em> du paramètre <span class="math inline">\(\theta\)</span> :</p>
<p>Après une phase de chauffe de longueure <span class="math inline">\(n\)</span>, nous recueillons un <span class="math inline">\(N\)</span>-échantillon de <span class="math inline">\(\theta\)</span> généré par cet algorithme de Metropolis-Hasting <span class="math inline">\(\left(\theta^{(n+1)}, ... , \theta^{(n+N)}\right)\)</span>. Appliquer la méthode de Monte-Carlo permet d’obtenir un estimateur de la moyenne <em>a posteriori</em> de <span class="math inline">\(\theta\)</span>.</p>
<p>L’estimateur bayésien pour une fonction de coût quadratique est la moyenne <em>a posteriori</em>, calculée par :
<span class="math display">\[\widehat{E}(\theta|\boldsymbol{x}) = \frac{1}{N} \sum_{k=1}^{N}\theta^{(n + k)}\]</span></p>
</div>
<p><br></p>
<ol start="4" style="list-style-type: decimal">
<li>Quel résultat théorique garantit sa convergence ? Expliquer brièvement.</li>
</ol>
<div class="Correction">
<p>La convergence de l’algorithme de Metropolis-Hastings est garantie par le <strong>théorème ergodique</strong>. L’algorithme de Metropolis-Hastings échantillonne les réalisations de <span class="math inline">\(\theta\)</span> à l’aide d’une chaine de Markov dont la distribution stationnaire est la loi <em>a posteriori</em> de <span class="math inline">\(\theta\)</span>. Le théorème ergodique permet donc d’appliquer la loi des grands nombres aux réalisations de cette chaîne.</p>
</div>
