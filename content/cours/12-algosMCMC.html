---
title: "Algorithmes MCMC"
linktitle: "Algorithmes MCMC"
date: "2020-11-30"
read_date: "2020-12-03"
menu:
  cours:
    parent: "III Calcul num√©rique pour l'analyse bay√©sienne"
    weight: 3
type: docs
bibliography: "../../static/bib/references.bib"
csl: "../../static/bib/chicago-fullnote-bibliography-no-bib.csl"
slides: "01-slides"
output:
  blogdown::html_page:
    toc: true
---


<div id="TOC">
<ul>
<li><a href="#cha√Ænes-de-markov">Cha√Ænes de Markov</a></li>
<li><a href="#√©chantillonnage-mcmc">√âchantillonnage MCMC</a></li>
</ul>
</div>

<p>Le principe des algorithmes MCMC est de construire une cha√Æne de Markov visitant l‚Äôespace des param√®tres dont la loi de probabilit√© invariante est la loi <em>a posteriori</em>.</p>
<div id="cha√Ænes-de-markov" class="section level3">
<h3>Cha√Ænes de Markov</h3>
<p>Une cha√Æne de Markov est un processus stochastique √† temps discret. On peut la d√©finir comme une suite de variables al√©atoires <span class="math inline">\(X_0\)</span>, <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span>, <span class="math inline">\(X_3\)</span>, ‚Ä¶ (toutes d√©finies sur le m√™me espace) poss√©dant la <strong>propri√©t√© de Markov</strong> (‚Äúsans m√©moire‚Äù) :
<span class="math display">\[p(X_i = x | X_0 = x_0, X_1 = x_1, \dots , X_{i-1} = x_{i-1}) = p(X_i = x |X_{i-1} = x_{i-1})\]</span>
L‚Äôensemble des valeurs possible pour <span class="math inline">\(X_i\)</span> est appel√© **espace d‚Äô√©tat* et est not√© <span class="math inline">\(E\)</span>.</p>
<p>Une cha√Æne de Markov est d√©termin√©e par deux param√®tres :</p>
<ol style="list-style-type: decimal">
<li>sa distribution initiale <span class="math inline">\(p(X_0)\)</span></li>
<li>son noyau de transition <span class="math inline">\(T(x,A) = p(X_i \in A | X_{i-1} = x)\)</span></li>
</ol>
<p>Dans la suite, on ne va consid√©rer que des chaines de Markov <strong>homog√®nes</strong>, c‚Äôest-√†-dire qui v√©rifient :
<span class="math display">\[p(X_{i+1} = x|X_i = y) = p(X_i = x|X_{i-1} = y)\]</span></p>
<div class="MyProp">
<p>Une cha√Æne de Markov est dite <strong>irr√©ductible</strong> : si tous les ensembles de probabilit√© non nulle peuvent √™tre atteints √† partir de tout point de d√©part (i.e.¬†tout √©tat est accessible √† partir de n‚Äôimporte quel autre).</p>
</div>
<p><br></p>
<div class="MyProp">
<p>Une cha√Æne de Markov est dite <strong>r√©currente</strong> si les trajectoires <span class="math inline">\((X_i)\)</span> passent une infinit√© de fois dans tout ensemble de probabilit√© non nulle de l‚Äôespace d‚Äô√©tat.</p>
</div>
<p><br></p>
<div class="MyProp">
<p>Une cha√Æne de Markov est dite <strong>ap√©riodique</strong> si rien n‚Äôinduit un comportement p√©riodique des trajectoires.</p>
</div>
<p><br></p>
<div class="MyDef">
<p>Une distribution de probabilit√© <span class="math inline">\(\tilde{p}\)</span> est appel√©e <strong>loi invariante</strong> (ou <strong>loi stationnaire</strong>) pour une cha√Æne de Markov si elle v√©rifie la propri√©t√© suivante : si <span class="math inline">\(X_i\)</span> suit <span class="math inline">\(\tilde{p}\)</span>, alors <span class="math inline">\(X_{i+1}\)</span> (et les √©l√©ments suivants) sont n√©cessairement distribu√©s suivant <span class="math inline">\(\tilde{p}\)</span>.</p>
</div>
<p><br></p>
<p>
<em>Remarque</em> : Une cha√Æne de Markov peut admettre plusieurs lois stationnaires.</p>
<p>
<em><strong>Th√©or√®me ergodique</strong> (espace infini)</em> : Une cha√Æne de Markov irr√©ductible et r√©currente positive (i.e.¬†le temps de retour moyen est fini) admet une unique loi de probabilit√© invariante <span class="math inline">\(\tilde{p}\)</span>. Si cette cha√Æne de Markov est de plus ap√©riodique, alors elle converge en loi vers <span class="math inline">\(\tilde{p}\)</span>.</p>
<div class="Example">
<p>B√©b√© üë∂<br />
Nous allons maintenant d√©velopper un exemple d‚Äôune cha√Æne de Markov √† espace d‚Äô√©tat discret. Supposons que l‚Äô√©tat de B√©b√©, suive √† chaque minute une cha√Æne de Markov discr√®te √† trois √©tats : dormir üò¥ (D), manger üçº (M), changer la couche üí© (C). Ainsi, son √©tat dans une minute ne d√©pend que de son √©tat actuel, et pas des minutes pr√©c√©dentes. Supposons que la matrice des probabilit√©s de transition soit alors la suivante :
<span class="math display">\[P = 
\begin{pmatrix}
X_i/X_{i+1} &amp; D &amp; M &amp; C \\
D &amp; 0.9 &amp; 0.05 &amp; 0.05\\
M &amp; 0.7 &amp; 0 &amp; 0.3\\
C &amp; 0.8 &amp; 0 &amp; 0.2\\
\end{pmatrix}\]</span></p>
<p>üëâ 1) Selon vous, la cha√Æne est-elle irr√©ductible? R√©currente? Ap√©riodique? (on pourra s‚Äôaider d‚Äôun graphique repr√©sentant les diff√©rents √©tats et leurs transitions)<br />
<span class="math inline">\(\quad\)</span><br />
</p>
<p>üëâ  2) Supposons que B√©b√© dorme (ü§û!). Que fait-il 2 min apr√®s? et 10 min apr√®s ?<br />
<span class="math display">\[x_0 = \phantom{\begin{pmatrix}
1 \\
0 \\
0 \\
\end{pmatrix}^T} \quad x_2=x_0P^2 = \phantom{\begin{pmatrix}
0.885 \\
0.045 \\
0.070 \\
\end{pmatrix}^T} 
\quad x_{10}=\phantom{x_2P^{8} = x_0P^{10} = \begin{pmatrix}
0.884 \\
0.044 \\
0.072 \\
\end{pmatrix}^T}\]</span></p>
<p>üëâ 3) Supposons maintenant se couche soit en train d‚Äô√™tre chang√©e. Que fait-il 10 min apr√®s ?<br />
<span class="math display">\[x_0 = \phantom{\begin{pmatrix}
0 \\
0 \\
1 \\
\end{pmatrix}^T} \quad x_{10} = \phantom{x_0P^{10} = \begin{pmatrix}
0.884 \\
0.044 \\
0.072 \\
\end{pmatrix}^T}\]</span></p>
<p>Ici la loi est ap√©riodique, r√©currente et irr√©ductible, il y a donc une loi stationnaire : <span class="math inline">\(\tilde{p}=\tilde{p}P\)</span>.</p>
</div>
<p><br></p>
</div>
<div id="√©chantillonnage-mcmc" class="section level3">
<h3>√âchantillonnage MCMC</h3>
<div id="algorithmes-mcmc-principe-g√©n√©ral" class="section level4">
<h4>Algorithmes MCMC : principe g√©n√©ral</h4>
<p>Le principe g√©n√©ral des algorithmes MCMC est le suivant : pour produire une approximation acceptable d‚Äôune int√©grale ou d‚Äôune autre fonctionnelle d‚Äôune distribution d‚Äôint√©r√™t (i.e.¬†la loi <em>a posteriori</em>), il suffit de g√©n√©rer une cha√Æne de Markov dont la distribution limite est la distribution d‚Äôint√©r√™t (i.e.¬†la loi <em>a posteriori</em>), puis d‚Äôy appliquer la m√©thode de Monte-Carlo.</p>
<p>Il faut donc avoir une <strong>double convergence</strong> :</p>
<ol style="list-style-type: decimal">
<li>la convergence de la cha√Æne de Markov vers sa distribution stationnaire : <span class="math inline">\(\forall X_0,\, X_n\underset{{n\to+\infty}}{\xrightarrow{\quad\mathcal{L}\quad}} \tilde{p}\)</span></li>
<li>la convergence de Monte-Carlo, une fois la distribution stationnaire atteinte : <span class="math inline">\(\frac{1}{N}\sum_{i=1}^N f(X_{n+i}) \underset{N\to+\infty}{\xrightarrow{\qquad~~}} \mathbb{E}[f(X)]\)</span></li>
</ol>
<p><span class="math display">\[\overbrace{X_0\rightarrow X_1 \rightarrow X_2\rightarrow \dots \rightarrow X_n}^\text{convergence de la cha√Æne de Markov}%
\rightarrow \overbrace{X_{n+1} \rightarrow X_{n+2} \rightarrow \dots \rightarrow X_{n+N}}^\text{√©chantillon de Monte-Carlo}\]</span></p>
<p>Les algorithmes MCMC utilisent une approche d‚Äôacceptation-rejet :</p>
<div class="Algo">
<ol style="list-style-type: decimal">
<li>Initialiser <span class="math inline">\(x^{(0)}\)</span></li>
<li>Pour <span class="math inline">\(t = 1, \dots, n + N\)</span> :
<ol style="list-style-type: lower-alpha">
<li>Proposer un nouveau candidat <span class="math inline">\(y^{(t)} \sim q(y^{(t)}|x^{(t-1)})\)</span></li>
<li>Accepter <span class="math inline">\(y^{(t)}\)</span> avec la probabilit√© <span class="math inline">\(\alpha(x^{(t-1)},y^{(t)})\)</span> :<br />
<span class="math inline">\(\quad x^{(t)}:= y^{(t)}\)</span><br />
Si <span class="math inline">\(t&gt;n\)</span>, ‚Äúsauver‚Äù <span class="math inline">\(x^{(t)}\)</span> (pour calculer la fonctionnelle d‚Äôint√©r√™t)
o√π <span class="math inline">\(q\)</span> est la loi instrumentale de proposition et <span class="math inline">\(\alpha\)</span> est la probabilit√© d‚Äôacceptation.</li>
</ol></li>
</ol>
</div>
<p><span class="math inline">\(\qquad\)</span><em>Sch√©ma g√©n√©ral des algorithmes MCMC</em></p>
<p>Pour la loi instrumentale de proposition <span class="math inline">\(q\)</span> il n‚Äôexiste pas de choix absolument optimal mais une infinit√© de lois possibles (certaines meilleures que d‚Äôautres). Afin de garantir la convergence vers la loi cible <span class="math inline">\(\tilde{p}\)</span> : (i) le support de <span class="math inline">\(q\)</span> doit contenir le support <span class="math inline">\(\tilde{p}\)</span>, (ii) <span class="math inline">\(q\)</span> ne doit pas g√©n√©rer de valeurs p√©riodiques. Id√©alement, on choisit <span class="math inline">\(q\)</span> de mani√®re √† ce que son calcul soit simple (par exemple on peut choisir <span class="math inline">\(q\)</span> sym√©trique).</p>
</div>
<div id="lalgorithme-de-metropolis-hastings" class="section level4">
<h4>L‚Äôalgorithme de Metropolis-Hastings</h4>
<p>L‚Äôalgorithme de Metropolis-Hastings est un algorithme tr√®s simple et tr√®s g√©n√©ral permettant d‚Äô√©chantillonner selon des lois uni- ou multi-dimensionnelles.</p>
<div class="Algo">
<ol style="list-style-type: decimal">
<li>Initialiser <span class="math inline">\(x^{(0)}\)</span><br />
</li>
<li>Pour <span class="math inline">\(t = 1, \dots, n + N\)</span> :
<ol style="list-style-type: lower-alpha">
<li>Proposer <span class="math inline">\(y^{(t)} \sim q(y^{(t)}|x^{(t-1)})\)</span></li>
<li>Calculer la probabilit√© d‚Äôacceptation<br />
<span class="math inline">\(\quad \alpha^{(t)} = \min\left\{1, \frac{\tilde{p}(y)}{q(y^{(t)}|x^{(t-1)})}\Big/\frac{\tilde{p}(x^{(t-1)})}{q(x^{(t-1)}|y^{(t)})}\right\}\)</span></li>
<li>√âtape d‚Äôacceptation-rejet : g√©n√©rer une valeur <span class="math inline">\(u^{(t)}\sim \mathcal{U}_{[0;1]}\)</span><br />
<span class="math inline">\(\quad x^{(t)}=\begin{cases}y^{(t)}\text{ si }u^{(t)} \leq \alpha^{(t)}\\x^{(t-1)}\text{ sinon}\end{cases}\)</span></li>
</ol></li>
</ol>
</div>
<p><br></p>
<p>On peut reformuler la probabilit√© d‚Äôacceptation <span class="math inline">\(\alpha^{(t)}\)</span> ainsi : <span class="math inline">\(\displaystyle\alpha^{(t)} = \min\left\{1, \frac{\tilde{p}(y^{(t)})}{\tilde{p}(x^{(t-1)})}\frac{q(x^{(t-1)}|y^{(t)})^{(t)}}{q(y^{(t)}|x^{(t-1)})}\right\}\)</span>. On voit donc qu‚Äôon peut la calculer en ne connaissant <span class="math inline">\(\tilde{p}\)</span> qu‚Äô√† une constante pr√®s, puisqu‚Äôelle se simplifie dans ce ratio.</p>
<p>Dans certains cas particuliers (tr√®s utilis√©s en pratique), le calcul de <span class="math inline">\(\alpha^{(t)}\)</span> est simplifi√© :</p>
<ul>
<li><strong>Metropolis-Hastings ind√©pendant</strong> : <span class="math inline">\(q(y^{(t)}|x^{(t-1)}) = q(y^{(t)})\)</span></li>
<li><strong>Metropolis-Hastings √† marche al√©atoire</strong> : <span class="math inline">\(q(y^{(t)}|x^{(t-1)}) = g(y^{(t)}-x^{(t-1)})\)</span>. Si <span class="math inline">\(g\)</span> est sym√©trique (<span class="math inline">\(g(-x) = g(x)\)</span>), alors le calcul de la probabilit√© d‚Äôacceptation <span class="math inline">\(\alpha^{(t)}\)</span> se simplifie : <span class="math inline">\(\require{cancel}\frac{\tilde{p}(y^{(t)})}{\tilde{p}(x^{(t-1)})}\frac{q(y^{(t)}|x^{(t-1)})}{q(x^{(t-1)}|y^{(t)})} = \frac{\tilde{p}(y^{(t)})}{\tilde{p}(x^{(t-1)})}\frac{\cancel{g(y^{(t)}-x^{(t-1)})}}{\cancel{g(x^{(t-1)}-y^{(t)})}} = \frac{\tilde{p}(y^{(t)})}{\tilde{p}(x^{(t-1)})}\)</span></li>
</ul>
<p>L‚Äôalgorithme de Metropolis-Hastings est un algorithme tr√®s simple et tr√®s g√©n√©ral permettant d‚Äô√©chantillonner de mani√®re uni- ou multi-dimensionnelle. Le choix de la distribution instrumentale est crucial, mais difficile, et a un impact consid√©rable sur les performances de l‚Äôalgorithme (un fort taux de rejet implique souvent des temps de calculs tr√®s importants). De plus, c‚Äôest un algorithme qui devient inefficace dans les probl√®mes de trop grande dimension. L‚Äôalgorithme du recuit-simul√© ainsi que l‚Äô√©chantillonneur de Gibbs sont des algorithmes permettant en partie de pallier √† certaines de ces limites.</p>
</div>
<div id="lalgorithme-du-recuit-simul√©" class="section level4">
<h4>L‚Äôalgorithme du recuit-simul√©</h4>
<p>Afin de d√©passer certaines limitations de l‚Äôalgorithme de Metropolis-Hastings, ici l‚Äôid√©e est de faire varier le calcul de la probabilit√© d‚Äôacceptation <span class="math inline">\(\alpha^{(t)}\)</span> au cours de l‚Äôalgorithme. La probabilit√© d‚Äôacceptation doit d‚Äôabord √™tre grande afin de bien explorer l‚Äôensemble de l‚Äôespace d‚Äô√©tat, puis diminuer au fur et √† mesure que l‚Äôalgorithme converge vers une r√©gion de l‚Äôespace, afin que les nouvelles valeurs accept√©es se concentrent autour du mode de convergence. Cela consiste √† introduire dans l‚Äôalgorithme de M√©tropolis-Hastings une ‚Äútemp√©rature‚Äù variant √† chaque it√©ration et not√©e <span class="math inline">\(T(t)\)</span> :</p>
<div class="Algo">
<ol style="list-style-type: decimal">
<li>Initialiser <span class="math inline">\(x^{(0)}\)</span></li>
<li>Pour <span class="math inline">\(t = 1,\dots, n + N\)</span> :
<ol style="list-style-type: lower-alpha">
<li>Proposer <span class="math inline">\(y^{(t)} \sim q(y^{(t)}|x^{(t-1)})\)</span></li>
<li>Calculer la probabilit√© d‚Äôacceptation<br />
<span class="math inline">\(\quad \alpha^{(t)} = \min\left\{1, \left(\frac{\tilde{p}(y^{(t)})}{\tilde{p}(x^{(t-1)})}\frac{q(x^{(t-1)}|y^{(t)})}{q(y^{(t)}|x^{(t-1)})}\right)^{\frac{1}{T(t)}}\right\}\)</span></li>
<li>√âtape d‚Äôacceptation-rejet : g√©n√©rer une valeur <span class="math inline">\(u^{(t)}\sim \mathcal{U}_{[0;1]}\)</span><br />
<span class="math inline">\(\quad x^{(t)}:=\begin{cases}y^{(t)}\text{ si }u^{(t)} \leq \alpha^{(t)}\\x^{(t-1)}\text{ sinon}\end{cases}\)</span></li>
</ol></li>
</ol>
</div>
<p><br></p>
<p>Par exemple, on peut prendre <span class="math inline">\(T(t) = T_0 \left(\frac{T_f}{T_0}\right)^\frac{t}{n}\)</span> avec <span class="math inline">\(T_0\)</span> la temp√©rature de base, <span class="math inline">\(n\)</span> le nombre d‚Äôit√©rations au-del√† duquel on pense √™tre proche de la convergence, <span class="math inline">\(T_f\)</span> la temp√©rature apr√®s <span class="math inline">\(n\)</span> it√©rations. Cet algorithme est particuli√®rement utile lors de la pr√©sence d‚Äôoptimums locaux.</p>
</div>
<div id="√©chantillonneur-de-gibbs" class="section level4">
<h4>√âchantillonneur de Gibbs</h4>
<p>Lorsque la dimension (de <span class="math inline">\(x\)</span>) augmente, il devient tr√®s difficile de proposer des valeurs probables dans les algorithmes utilisant la strat√©gie d‚Äôacceptation-rejet. L‚Äôid√©e de l‚Äô√©chantillonneur de Gibbs est de g√©n√©rer <span class="math inline">\(x\)</span> coordonn√©e par coordonn√©e, en conditionnant sur les derni√®res valeurs obtenues. Il faut donc que <span class="math inline">\(x\)</span> admette une d√©composition telle que <span class="math inline">\(x=(x_1, \dots, x_d)\)</span>, et que les distributions <span class="math inline">\(p(x_i | x_1, \dots, x_{i-1}, x_{i+1}, \dots, x_d)\)</span> soit connues et ais√©ment simulables. L‚Äô√©chantillonneur de Gibbs est une suite d‚Äô√©tape de Metropolis-Hastings (coordonn√©e par coordonn√©e), les propositions √©chantillonn√©es sont toujours accept√©e (<span class="math inline">\(\alpha = 1\)</span>). On obtient cette acceptation inconditionnelle en imposant les lois de propositions : il s‚Äôagit de la distribution conditionnelle respective de chaque coordonn√©e. On peut donc voir l‚Äô√©chantillonneur de Gibbs comme un algorithme de r√©actualisation composante par composante :</p>
<div class="Algo">
<ol style="list-style-type: decimal">
<li>Initialiser <span class="math inline">\(x^{(0)}= (x_1^{(0)}, \dots, x_d^{(0)})\)</span></li>
<li>Pour <span class="math inline">\(t = 1,\dots, n + N\)</span> :
<ol style="list-style-type: lower-alpha">
<li>G√©n√©rer <span class="math inline">\(x_1^{(t)} \sim p(x_1 | x_2^{(t-1)}, \dots, x_d^{(t-1)})\)</span></li>
<li>G√©n√©rer <span class="math inline">\(x_2^{(t)} \sim p(x_2 | x_1^{(t)}, x_3^{(t-1)}, \dots, x_d^{(t-1)})\)</span></li>
<li>‚Ä¶</li>
<li>G√©n√©rer <span class="math inline">\(x_i^{(t)} \sim p(x_i | x_1^{(t)}, \dots, x_{i-1}^{(t)}, x_{i+1}^{(t-1)}, \dots, x_d^{(t-1)})\)</span></li>
<li>‚Ä¶</li>
<li>G√©n√©rer <span class="math inline">\(x_d^{(t)} \sim p(x_d | x_1^{(t)}, \dots, x_{d-1}^{(t)})\)</span></li>
</ol></li>
</ol>
</div>
<p><br></p>
<p>
<em>Remarque</em> : si l‚Äôon ne conna√Æt pas certaines lois conditionnelles pour certaines coordonn√©es, on peut tout de les √©chantillonner en introduisant une √©tape d‚Äôacceptation-rejet pour cette coordonn√©e uniquement. On parle alors d‚Äôalgorithme de M√©tropolis √† l‚Äôint√©rieur de Gibbs (<em>Metropolis within Gibbs</em>).</p>
</div>
</div>
