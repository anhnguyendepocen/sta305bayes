---
title: ""
linktitle: "Une difficile estimation de la distribution a posteriori"
date: "2020-11-30"
read_date: "2020-12-03"
menu:
  cours:
    parent: "III Calcul numérique pour l'analyse bayésienne"
    weight: 1
type: docs
bibliography: "../../static/bib/references.bib"
csl: "../../static/bib/chicago-fullnote-bibliography-no-bib.csl"
slides: "01-slides"
output:
  blogdown::html_page:
    toc: false
---



<div id="une-difficile-estimation-de-la-distribution-a-posteriori" class="section level2">
<h2>Une difficile estimation de la distribution <em>a posteriori</em></h2>
<div id="paramètres-multidimensionnels" class="section level3">
<h3>Paramètres multidimensionnels</h3>
<p>Dans les applications réelles il y a en général plusieurs paramètres. On a donc un vecteur de paramètres <span class="math inline">\(\boldsymbol{\theta} = (\theta_1,\dots, \theta_d)\)</span>. La formule de Bayes donnant la distribution <em>a posteriori</em> à partir de la loi <em>a priori</em> et de la vraisemblance est toujours valable dans ce cas : elle donne la loi <em>a posteriori</em> conjointe des <span class="math inline">\(d\)</span> paramètres. Toute l’information est contenue dans cette loi conjointe. Mais son calcul numérique n’est pas toujours facile, notamment dans des modèles complexes, et dans certains modèles même la vraisemblance est difficile à calculer. De plus, pour obtenir la distribution <em>a posteriori</em> conjointe, il faut aussi calculer la constante d’intégration <span class="math inline">\(f(\boldsymbol{y}) = \textstyle \int_{\Theta^d} f(\boldsymbol{y}|\boldsymbol{\theta})\pi(\boldsymbol{\theta})\,\text{d}\boldsymbol{\theta}\)</span>. Une solution analytique n’est disponible que dans des cas très particuliers (notamment lors de l’utilisation de lois conjuguées); et dans la majorité des cas pratiques, l’intégrale doit en fait être calculée numériquement. Si <span class="math inline">\(\theta\)</span> est de dimension <span class="math inline">\(d\)</span>, il s’agit de calculer une intégrale de multiplicité <span class="math inline">\(d\)</span>, ce qui devient difficile lorsque <span class="math inline">\(d\)</span> est grand (les problèmes numériques apparaissent dès que <span class="math inline">\(d&gt; 4\)</span>).</p>
<p>Un problème encore plus difficile surgit lorsque l’on veut tirer des conclusions à partir de cette distribution <em>a posteriori</em> conjointe. En général nous sommes intéressés par les valeurs possibles pour chaque paramètre. C’est à dire que l’on a besoin des distributions marginales, unidimensionnelles, de chaque paramètre. Pour les obtenir il faut là aussi intégrer la distribution conjointe (et ce <span class="math inline">\(d\)</span> fois s’il y a <span class="math inline">\(d\)</span> paramètres). Le problème est d’autant plus difficile qu’il faut calculer ces intégrales pour chaque valeur possible du paramètre pour reconstituer toute la densité <em>a posteriori</em> numériquement. Dans les problèmes complexes un calcul suffisamment précis de ces intégrales parait impossible et l’on a recours en général à des algorithmes basés sur des simulations d’échantillonnage, en particulier les algorithmes de Monte-Carlo par chaînes de Markov (dits “<em>Markov chain Monte Carlo</em>” — MCMC).</p>
</div>
<div id="statistique-bayésienne-computationnelle" class="section level3">
<h3>Statistique bayésienne computationnelle</h3>
<p>Identifier la loi <em>a posteriori</em> apparaît simple en théorie grâce au théorème de Bayes. Mais en pratique le calcul de l’intégrale au dénominateur s’avère souvent extrêmement difficile. Trouver une expression analytique n’est possible que dans quelques cas bien particuliers, et l’évaluation numérique peut se révéler tout aussi difficile, notamment lorsque la dimension de l’espace des paramètres augmente.</p>
<p>La statistique bayésienne computationnelle cherche des solutions pour pouvoir estimer la distribution <em>a posteriori</em>, y compris lorsqu’on ne connaît que le numérateur dans le théorème de Bayes (loi <em>a posteriori</em> non-normée). Les principales méthodes utilisées s’appuient sur des algorithmes d’échantillonnage permettant de générer un échantillon distribué selon la loi {a posteriori}. On peut distinguer deux grands types parmi ces algorithmes : i) d’abord les méthodes d’échantillonnage directes, où l’on génère un échantillon à partir d’une loi simple (par exemple uniforme), que l’on transforme afin que le résultat soit distribué selon la loi <em>a posteriori</em> ; ii) les méthodes de Monte-Carlo par chaînes de Markov (MCMC), où l’on construit une chaîne de Markov sur l’espace des paramètres dont la loi invariante correspond à la loi <em>a posteriori</em>.</p>
</div>
<div id="méthode-de-monte-carlo" class="section level3">
<h3>Méthode de Monte-Carlo</h3>
<p>Monte-Carlo (1955) est le nom crypté d’un projet de John von Neumann et Stanislas Ulam au <em>Los Alamos Scientific Laboratory</em> visant à utiliser des nombres aléatoires pour estimer des quantités difficiles (ou impossible) à calculer analytiquement.</p>
<p>En s’appuyant sur la loi des grands nombres, il s’agit d’obtenir un échantillon dit “de Monte-Carlo” permettant de calculer divers fonctionnelles à partir de la distribution de probabilité suivie par l’échantillon. En effet <span class="math inline">\(\mathbb{E}[f(X)] = \int_x f(x)p_X(x)dx\)</span>. Or grâce à la loi des grands nombres, on a : <span class="math inline">\(\mathbb{E}[f(X)] = \frac{1}{N}\sum_i f(x_i)\)</span> à condition que les <span class="math inline">\(x\)</span> forment un échantillon <span class="math inline">\(iid\)</span> selon la loi de <span class="math inline">\(X\)</span>. On peut ainsi estimer un certain nombre d’intégrales, à condition d’être capable d’échantillonner selon <span class="math inline">\(p(x)\)</span>.</p>
<div class="Example">
<p>Estimation du nombre <span class="math inline">\(\pi=3,14\dots\)</span> à l’aide de nombres aléatoires.</p>
<p><img src="../../static/cours/IllusRoulette.png" width="944" /></p>
<ol style="list-style-type: decimal">
<li>La probabilité d’être dans le cercle plutôt que dans le carré est le rapport entre la surface du cercle et celle du carré : <span class="math inline">\(p_C= \frac{\pi R^2}{(2R)^2}=\frac{\pi}{4}\)</span></li>
<li>On génère <span class="math inline">\(n\)</span> points <span class="math inline">\(\left((x_{11},x_{21}),\dots,(x_{1n},x_{2n})\right)=(P_1,\dots,P_n)\)</span> dans le repère <span class="math inline">\(36\times 36\)</span>, à l’aide de la roulette qui génère les coordonnées une à une.</li>
<li>Placer ces points dans le repère et compter le nombre qui sont dans le cercle.</li>
<li>Calculer le ratio (probabilité estimée d’être dans le cercle) : <span class="math display">\[\widehat{p}_C=\frac{\sum P_i\in cercle}{n}\]</span></li>
</ol>
<p>Si <span class="math inline">\(n=1000\)</span> et que l’on trouve <span class="math inline">\(786\)</span> points sont dans le cercle, alors on a <span class="math inline">\(\textstyle\widehat{\pi} = 4\times \frac{786}{1000}=3,144\)</span>. On pourrait améliorer notre estimation en augmentant la résolution de notre grille, et aussi en augmentant notre nombre de points <span class="math inline">\(n\)</span>. En effet, on a <span class="math inline">\(\lim_{n\to+\infty}\widehat{p}_C = p_C\)</span> d’après la loi des grands nombres.</p>
<p>Ainsi on a construit un <strong>échantillon de Monte-Carlo</strong> à partir de cet échantillon on peut calculer de nombreuses fonctionnelles, notamment <span class="math inline">\(\pi\)</span> qui correspond à 4 fois la probabilité d’être dans le cercle.</p>
<p>De manière analogue, les méthodes d’échantillonnage directes ou par MCMC cherchent à construire un échantillon de Monte-Carlo de la distribution <em>a posteriori</em>, afin de calculer un certain nombre de fonctionnelles (Moyennes a posteriori, intervalles de crédibilité, etc.)…</p>
</div>
</div>
</div>
