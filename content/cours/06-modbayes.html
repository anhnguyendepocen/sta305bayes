---
title: ""
linktitle: "Construction d'un mod√®le bay√©sien"
date: "2020-11-30"
read_date: "2020-12-01"
menu:
  cours:
    parent: "II Mod√©lisation Bay√©sienne"
    weight: 3
type: docs
bibliography: "../../static/bib/references.bib"
csl: "../../static/bib/chicago-fullnote-bibliography-no-bib.csl"
slides: "01-slides"
output:
  blogdown::html_page:
    toc: false
---



<div id="construction-dun-mod√®le-bay√©sien" class="section level2">
<h2>Construction d‚Äôun mod√®le bay√©sien</h2>
<p>La premi√®re √©tape dans la construction d‚Äôun mod√®le est toujours d‚Äôidentifier la question √† laquelle on souhaite r√©pondre. Une fois cette √©tape accomplie, il s‚Äôagit de d√©terminer quelles observations sont disponibles, et vont pouvoir nous informer dans notre r√©ponse √† la question d‚Äôint√©r√™t.</p>
<div id="mod√®le-d√©chantillonnage" class="section level3">
<h3>Mod√®le d‚Äô√©chantillonnage</h3>
<p>Notons <span class="math inline">\(\boldsymbol{y}\)</span> les observations dont nous disposons. De la m√™me mani√®re que dans un mod√®le fr√©quentiste, une mod√©lisation bay√©sienne param√©trique consiste √† d‚Äôabord proposer un mod√®le probabiliste pour ces observations : <span class="math inline">\(Y_i\overset{iid}{\sim} f(y|\theta)\)</span>. On appelle ce dernier ‚Äúmod√®le d‚Äô√©chantillonnage‚Äù.</p>
<p>üëâ  Dans l‚Äôapplication historique, Laplace a propos√© un mod√®le d‚Äô√©chantillonnage bas√© sur la loi ‚Ä¶</p>
<blockquote>
<p>‚Ä¶<br />
‚Ä¶</p>
</blockquote>
</div>
<div id="distribution-a-priori" class="section level3">
<h3>Distribution <em>a priori</em></h3>
<p>Dans la mod√©lisation bay√©sienne, par rapport √† la mod√©lisation fr√©quentiste, on ajoute une loi de probabilit√© (d√©finie sur l‚Äôespace <span class="math inline">\(\Theta\)</span> des param√®tres), appel√©e distribution <em>a priori</em> :
<span class="math display">\[\begin{align*}
\theta &amp;\sim \pi(\theta)\\
Y_i|\theta &amp;\overset{iid}{\sim} f(y|\theta)
\end{align*}\]</span>
On va donc traiter <span class="math inline">\(\theta\)</span> comme une variable al√©atoire, mais qui n‚Äôest jamais observ√©e !</p>
<p>üëâ  Dans l‚Äôapplication historique, Laplace a d‚Äôabord envisag√© un <em>a priori</em> ‚Ä¶</p>
<blockquote>
<p>‚Ä¶<br />
‚Ä¶</p>
</blockquote>
</div>
<div id="distribution-a-posteriori" class="section level3">
<h3>Distribution <em>a posteriori</em></h3>
<p>L‚Äôobjet d‚Äôune telle mod√©lisation bay√©sienne est la distribution des param√®tres <em>a posteriori</em>, c‚Äôest-√†-dire la loi de <span class="math inline">\(\theta\)</span> conditionnellement aux observations : <span class="math inline">\(p(\theta|\boldsymbol{Y} = \boldsymbol{y})\)</span>, appel√©e distribution <em>a posteriori</em>. Elle se calcule √† partir du mod√®le d‚Äô√©chantillonnage <span class="math inline">\(f(y|\theta)\)</span> ‚Äî √† partir duquel on obtient la vraisemblance <span class="math inline">\(f(\boldsymbol{y}|\theta)\)</span> pour toutes les observations ‚Äî et de la loi <em>a priori</em> <span class="math inline">\(\pi(\theta)\)</span> par le th√©or√®me de Bayes :
<span class="math display">\[p(\theta|\boldsymbol{y}) = \frac{f(\boldsymbol{y}|\theta)\pi(\theta)}{f(\boldsymbol{y})}\]</span>
o√π <span class="math inline">\(f(\boldsymbol{y}) = \int f(\boldsymbol{y}|\theta)\pi(\theta)\,\text{d}\theta\)</span> est la loi marginale de <span class="math inline">\(\boldsymbol{Y}\)</span>.</p>
<div id="exemple-avec-un-a-priori-uniforme" class="section level4">
<h4>Exemple avec un <em>a priori</em> uniforme</h4>
<p>üëâ  Dans l‚Äôapplication historique, la vraisemblance est donc : ‚Ä¶</p>
<blockquote>
<p>‚Ä¶<br />
‚Ä¶<br />
‚Ä¶</p>
</blockquote>
<p>üëâ  On obtient alors la loi <em>a posteriori</em> suivante : ‚Ä¶</p>
<blockquote>
<p>‚Ä¶<br />
‚Ä¶<br />
‚Ä¶<br />
‚Ä¶</p>
</blockquote>
<p>Malheureusement, cette int√©grale (dite ‚Äúincompl√®te‚Äù) n‚Äôa pas de solution analytique.</p>
<p>Une approximation par la loi normale a cependant permis √† Laplace de conclure que la probabilit√© de naissance d‚Äôun fille est inf√©rieure √† celle d‚Äôun gar√ßon,<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> puisqu‚Äôil obtint : <span class="math inline">\(P(\theta\geq 0.5|\boldsymbol{y})\approx 1.15~10^{-42}\)</span></p>
</div>
<div id="exemple-de-la-conjugaison-de-la-loi-beta" class="section level4">
<h4>Exemple de la conjugaison de la loi Beta</h4>
<p>Imaginons maintenant que l‚Äôon utilise une autre loi <em>a priori</em>, par exemple la loi <span class="math inline">\(\text{Beta}(\alpha, \beta)\)</span> dont la densit√© s‚Äô√©crit : <span class="math inline">\(\textstyle f(\theta)=\frac{(\alpha +\beta -1)!}{(\alpha-1)!(\beta -1)!}\theta^{\alpha-1}(1-\theta)^{\beta-1}\)</span> (pour <span class="math inline">\(\alpha &gt;0\)</span> et <span class="math inline">\(\beta&gt;0\)</span>).</p>
<p><img src="/cours/06-modbayes_files/figure-html/Beta%20density-1.png" width="63%" /></p>
<p>üëâ  On remarque que la loi uniforme est un cas particulier de la loi Beta lorsque <span class="math inline">\(\alpha\)</span> et <span class="math inline">\(\beta\)</span> valent tous les deux 1. Si on re-calcule le <em>posterior</em> avec un <em>a priori</em> <span class="math inline">\(\pi = \text{Beta}(\alpha, \beta)\)</span>, on obtient facilement :</p>
<blockquote>
<p>‚Ä¶</p>
</blockquote>
<p>üëâ  On reconnait, √† une constante de normalisation pr√®s ‚Ä¶</p>
<blockquote>
<p>‚Ä¶</p>
</blockquote>
<p>üëâ  On en d√©duit donc que ‚Ä¶</p>
<blockquote>
<p><span class="math inline">\(\theta|\textbf{y}\sim\)</span> ‚Ä¶</p>
</blockquote>
<p>On dit que l‚Äôon est dans une situation de <strong>distributions conjugu√©es</strong> car les distributions <em>a posteriori</em> et <em>a priori</em> appartiennent √† la m√™me famille param√©trique.</p>
<p>On peut maintenant √©valuer l‚Äôimpact de cet <em>a priori</em> Beta sur notre r√©sultat en fonction du choix des hyperparam√®tres <span class="math inline">\(\alpha\)</span> et <span class="math inline">\(\beta\)</span>.</p>
<table>
<caption>Pour 493 472 naissances dont 241 945 filles</caption>
<colgroup>
<col width="35%" />
<col width="25%" />
<col width="39%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Interpr√©tation de l‚Äô<em>a priori</em></th>
<th align="left">Param√®tres de la Beta</th>
<th align="center"><span class="math inline">\(P(\theta\geq 0.5|\boldsymbol{y})\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">#gar√ßons <span class="math inline">\(&gt;\)</span> #filles</td>
<td align="left"><span class="math inline">\(\alpha=0.1, \beta=3\)</span></td>
<td align="center"><span class="math inline">\(1.08~10^{-42}\)</span></td>
</tr>
<tr class="even">
<td align="left">#gar√ßons <span class="math inline">\(&lt;\)</span> #filles</td>
<td align="left"><span class="math inline">\(\alpha=3,\phantom{.1}\beta=0.1\)</span></td>
<td align="center"><span class="math inline">\(1.19~10^{-42}\)</span></td>
</tr>
<tr class="odd">
<td align="left">#gar√ßons <span class="math inline">\(=\)</span> #filles</td>
<td align="left"><span class="math inline">\(\alpha=4,\phantom{.1}\beta=4\)</span></td>
<td align="center"><span class="math inline">\(1.15~10^{-42}\)</span></td>
</tr>
<tr class="even">
<td align="left">#gar√ßons <span class="math inline">\(\neq\)</span> #filles</td>
<td align="left"><span class="math inline">\(\alpha=0.1,\beta=0.1\)</span></td>
<td align="center"><span class="math inline">\(1.15~10^{-42}\)</span></td>
</tr>
<tr class="odd">
<td align="left">non informatif</td>
<td align="left"><span class="math inline">\(\alpha=1,\phantom{.1}\beta=1\)</span></td>
<td align="center"><span class="math inline">\(1.15~10^{-42}\)</span></td>
</tr>
</tbody>
</table>
<p>On remarque que l‚Äô<em>a priori</em> ne semble pas influer sur notre r√©sultat ici. C‚Äôest parce que l‚Äôon dispose de beaucoup d‚Äôobservations. Le poids de la distribution <em>a priori</em> dans la ditribution <em>a posteriori</em> devient alors tr√®s faible en regard de l‚Äôinformation apport√©e par les observations. Si l‚Äôon imagine que l‚Äôon avait observ√© seulement 20 naissances, dont 9 filles, on note alors une influence de l‚Äô<em>a priori</em> bien plus grande.</p>
<table>
<caption>Pour 20 naissances dont 9 filles</caption>
<colgroup>
<col width="35%" />
<col width="25%" />
<col width="39%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Interpr√©tation de l‚Äô<em>a priori</em></th>
<th align="left">Param√®tres de la Beta</th>
<th align="center"><span class="math inline">\(P(\theta\geq 0.5|\boldsymbol{y})\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">#gar√ßons <span class="math inline">\(&gt;\)</span> #filles</td>
<td align="left"><span class="math inline">\(\alpha=0.1, \beta=3\)</span></td>
<td align="center"><span class="math inline">\(0.39\)</span></td>
</tr>
<tr class="even">
<td align="left">#gar√ßons <span class="math inline">\(&lt;\)</span> #filles</td>
<td align="left"><span class="math inline">\(\alpha=3,\phantom{.1}\beta=0.1\)</span></td>
<td align="center"><span class="math inline">\(0.52\)</span></td>
</tr>
<tr class="odd">
<td align="left">#gar√ßons <span class="math inline">\(=\)</span> #filles</td>
<td align="left"><span class="math inline">\(\alpha=4,\phantom{.1}\beta=4\)</span></td>
<td align="center"><span class="math inline">\(0.46\)</span></td>
</tr>
<tr class="even">
<td align="left">#gar√ßons <span class="math inline">\(\neq\)</span> #filles</td>
<td align="left"><span class="math inline">\(\alpha=0.1,\beta=0.1\)</span></td>
<td align="center"><span class="math inline">\(0.45\)</span></td>
</tr>
<tr class="odd">
<td align="left">non informatif</td>
<td align="left"><span class="math inline">\(\alpha=1,\phantom{.1}\beta=1\)</span></td>
<td align="center"><span class="math inline">\(0.45\)</span></td>
</tr>
</tbody>
</table>
<p><img src="/cours/06-modbayes_files/figure-html/Posterior%20density%20Beta-1.png" width="95%" /></p>
</div>
</div>
<div id="la-question-√©pineuse-du-choix-de-la-distribution-a-priori" class="section level3">
<h3>La question √©pineuse du choix de la distribution <em>a priori</em></h3>
<p>Un point essentiel de l‚Äôapproche bay√©sienne est donc de donner une distribution aux param√®tres. Dans l‚Äôinf√©rence bay√©sienne, on part d‚Äôune distribution <em>a priori</em>, et l‚Äôinformation contenue dans les observations est utilis√©e pour obtenir la distribution <em>a posteriori</em>. La distribution <em>a priori</em> apporte de la flexibilit√© par rapport √† un mod√®le fr√©quentiste, en permettant d‚Äôincorporer dans le mod√®le de la connaissance externe. Cela peut par exemple permettre de r√©soudre des probl√®mes d‚Äôidentifiabilit√© parfois rencontr√©s par une approche purement fr√©quentiste lorsque l‚Äôinformation apport√©e par les observations ne suffit pas pour estimer tous les param√®tres d‚Äôint√©r√™t.</p>
<p>C‚Äôest donc un grand avantage de l‚Äôapproche bay√©sienne. Mais d‚Äôun autre c√¥t√©, le choix de cette distribution <em>a priori</em> des param√®tres introduit une subjectivit√© intrins√®que dans l‚Äôanalyse, qui peut √™tre d√©cri√©e. Par exemple un statisticien travaillant pour un laboratoire pharmaceutique pourrait choisir une loi a priori donnant une forte probabilit√© qu‚Äôun m√©dicament soit efficace, ce qui influencera n√©cessairement le r√©sultat. Le choix (ou l‚Äô√©licitation) de la distribution <em>a priori</em> est donc d√©licat.</p>
<p>Notons tout d‚Äôabord deux points th√©oriques :</p>
<ul>
<li>le support de la distribution <em>a posteriori</em> doit √™tre inclus dans celui de la distribution <em>a priori</em>. En d‚Äôautres termes, si <span class="math inline">\(\pi(\theta) = 0\)</span>, alors <span class="math inline">\(p(\theta|\boldsymbol{y}) = 0\)</span>.</li>
<li>en g√©n√©ral on suppose l‚Äôind√©pendance des diff√©rents param√®tres sous la loi <em>a priori</em> (quand il y a plus d‚Äôun param√®tre ‚Äî ce qui est presque toujours le cas dans les applications) ce qui permet d‚Äô√©liciter les lois <em>a priori</em> param√®tre par param√®tre.</li>
</ul>
<div id="√©licitation-de-la-distribution-a-priori" class="section level4">
<h4>√âlicitation de la distribution <em>a priori</em></h4>
<p>Il y a des strat√©gies pour communiquer avec les experts non-statisticiens pour transformer leurs <strong>connaissances</strong> <em>a priori</em> en <strong>distribution</strong> <em>a priori</em>.</p>
<p>La m√©thode la plus simple est de demander aux experts de donner des poids ou des probabilit√©s √† des intervalles de valeurs : c‚Äôest la m√©thode des histogrammes. Cependant, quand le param√®tre peut prendre des valeurs sur un ensemble non-born√© cette m√©thode risque de donner un <em>a priori</em> nul pour des valeurs du param√®tre qui sont n√©anmoins possibles‚Ä¶ </p>
<p>Une autre approche est de se donner une famille param√©trique de distributions <span class="math inline">\(p(\theta|\eta)\)</span> et de choisir <span class="math inline">\(\eta\)</span> de telle sorte que la distribution <em>a priori</em> soit en accord avec ce que pensent les experts pour certaines caract√©ristiques. Par exemple on pourra faire en sorte que les deux premiers moments (moyenne et variance), ou bien des quantiles simples (comme les quartiles), co√Øncident avec leurs vues. Cela r√©sout le probl√®me de support soulev√© par la m√©thode des histogrammes. Cependant le choix de la famille param√©trique peut avoir de l‚Äôimportance. Par exemple une distribution normale <span class="math inline">\(\mathcal{N}(0 ; 2,19)\)</span> a les m√™me quartiles qu‚Äôune distribution de Cauchy <span class="math inline">\(\mathcal{C}(0;1)\)</span>, √† savoir <span class="math inline">\(-1;0;1\)</span>. Or ces deux lois <em>a priori</em> peuvent donner des distributions <em>a posteriori</em> assez diff√©rentes. Une strat√©gie pour d√©terminer les quartiles est de poser les questions suivantes :</p>
<ul>
<li>pour la m√©diane : <em>Pouvez-vous d√©terminer une valeur telle que <span class="math inline">\(\theta\)</span> a autant de chances de se trouver au-dessus qu‚Äôau-dessous ?</em></li>
<li>puis pour le premier quartile : <em>Supposons que l‚Äôon vous dise que <span class="math inline">\(\theta\)</span> est en dessous de [telle valeur m√©diane], pouvez-vous alors d√©terminer une nouvelle valeur telle que <span class="math inline">\(\theta\)</span> ait autant de chances de se trouver au-dessus qu‚Äôau-dessous ?</em></li>
<li>de fa√ßon similaire on d√©termine le troisi√®me quartile‚Ä¶</li>
</ul>
<p>Des logiciels existent pour aider √† l‚Äô√©licitation des lois <em>a priori</em> par des experts : voir par exemple l‚Äôoutil acad√©mique SHELF.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></p>
<p>On peut √©galement √©liciter des lois <em>a priori</em> d‚Äôapr√®s les donn√©es de la litt√©rature. L‚Äôid√©e est de d√©finir les moments de la distribution <em>a priori</em> tels qu‚Äôils donnent une probabilit√© raisonnable aux valeurs du param√®tre qui ont √©t√© recens√©es dans la litt√©rature. Si on propose un <em>a priori</em> normal de loi <span class="math inline">\(\mathcal{N}(\mu, \sigma^2)\)</span>, on peut par exemple choisir <span class="math inline">\(\mu\)</span> et <span class="math inline">\(\sigma\)</span> de telle sorte que la plus petite valeur donn√©e dans la litt√©rature soit √©gale √† <span class="math inline">\(\mu - 1.96\sigma\)</span> et la plus grande √† <span class="math inline">\(\mu + 1.96\sigma\)</span> . Une approche plus √©labor√©e est de maximiser la vraisemblance des valeurs de la litt√©rature‚Ä¶</p>
</div>
<div id="la-qu√™te-des-a-priori-non-informatifs" class="section level4">
<h4>La qu√™te des <em>a priori</em> non-informatifs</h4>
<p>Pour certains param√®tres (ou pour tous les param√®tres) il est courant que l‚Äôon n‚Äôait pas de connaissance <em>a priori</em>. On cherche alors √† d√©finir une distribution ‚Äúnon-informative‚Äù. Par exemple si le param√®tre est la probabilit√© qu‚Äôun pi√®ce de monnaie tombe sur pile ou face, une loi non-informative pourrait √™tre la loi uniforme sur [0;1] (le choix historique de Bayes en 1763). Cependant deux difficult√©s majeures apparaissent :</p>
<ul>
<li><strong>Lois impropres</strong></li>
</ul>
<p>La premi√®re difficult√© est que l‚Äôon peut √™tre amen√© √† consid√©rer des lois impropres. Une loi impropre est d√©finie par une densit√© dont la somme ne fait pas √† 1. Par exemple pour un param√®tre de moyenne d‚Äôune loi normale, il peut semble naturel de donner une densit√© constante <span class="math inline">\(\pi(\theta) = c\)</span> (i.e.¬†toutes les valeurs possibles sur <span class="math inline">\(]-\infty, +\infty[\)</span> ont la m√™me probabilit√©). Bien s√ªr <span class="math inline">\(\int_{-\infty}^\infty c\,\text{d}\theta = \infty\)</span>, et un tel choix ne d√©finit donc pas une loi de probabilit√© ! Il reste cependant <strong>admissible car la loi <em>a posteriori</em> est</strong> (la plupart du temps) <strong>propre</strong>. En effet nous avons alors :
<span class="math display">\[p(\theta|y) = \frac{f(y|\theta)c}{\int f(y|\theta)c\,\text{d}\theta}\]</span>
Si <span class="math inline">\(\int f(y|\theta)c\,\text{d}\theta = K\)</span> (comme c‚Äôest souvent le cas), alors <span class="math inline">\(\textstyle p(\theta|y) = \frac{f(y|\theta)}{K}\)</span> est une densit√© propre (i.e.¬†qui somme √† <span class="math inline">\(1\)</span>).</p>
<ul>
<li><strong>Lois non-invariantes</strong></li>
</ul>
<p>La seconde difficult√© vient de la non-invariance de la distribution uniforme pour des transformations non-lin√©aires des param√®tres. En effet si on fait une transformation des param√®tres <span class="math inline">\(\gamma = g(\theta)\)</span> la densit√© de <span class="math inline">\(\gamma\)</span> s‚Äô√©crit : <span class="math inline">\(\pi(\gamma) = |J|\,\pi(\theta)\)</span>, o√π <span class="math inline">\(|J|\)</span> est le Jacobien de la transformation, c‚Äôest-√†-dire le d√©terminant de la matrice jacobienne <span class="math inline">\(\textstyle J = \frac{\partial g^{-1}(\gamma)}{\partial \gamma}\)</span>. Par exemple si on prend une densit√© uniforme √©gale √† 1 pour <span class="math inline">\(\theta\)</span> sur <span class="math inline">\((0, +\infty)\)</span> et que l‚Äôon fait la transformation <span class="math inline">\(\gamma = \log(\theta)\)</span>, on a <span class="math inline">\(g^{-1}(\gamma) = e^\gamma\)</span> et <span class="math inline">\(|J| = e^\gamma\)</span>. Donc on a <span class="math inline">\(\pi(\gamma) = e^\gamma\)</span>, ce qui n‚Äôest pas la caract√©risation d‚Äôune loi uniforme. D‚Äôo√π le paradoxe suivant : si la loi uniforme pour <span class="math inline">\(\theta\)</span> traduit une absence totale de connaissance <em>a priori</em> sur <span class="math inline">\(\theta\)</span>, on devrait avoir aussi une absence totale d‚Äôinformation <em>a priori</em> sur <span class="math inline">\(\gamma\)</span>, ce qui devrait se traduire par une loi uniforme sur <span class="math inline">\(\gamma\)</span>. Or ce ne peut √™tre le cas. Donc la loi uniforme ne peut pas √™tre d‚Äôune mani√®re g√©n√©rale la loi repr√©sentant une absence totale de connaissance <em>a priori</em>. C‚Äôest un argument central qui a conduit Fisher, en 1922, √† proposer les estimateurs du maximum de vraisemblance, poss√©dant eux une propri√©t√© d‚Äôinvariance pour des transformations non-lin√©aires des param√®tres.</p>
<p><strong>NB :</strong> Ceci ne veut pas dire que l‚Äôon ne puisse pas prendre une loi uniforme comme <em>a priori</em>, mais il faut avoir conscience que la loi uniforme ne vaut que pour une certaine param√©trisation‚Ä¶</p>
<p>Face √† ces difficult√©s, diff√©rentes solutions ont √©t√© propos√©es. Elles ont montr√© qu‚Äôil n‚Äôy a pas de loi <em>a priori</em> compl√®tement non-informative, mais on peut consid√©rer certaines lois comme <strong>faiblement informatives</strong>.</p>
</div>
<div id="la-loi-a-priori-de-jeffreys" class="section level4">
<h4>La loi <em>a priori</em> de Jeffreys</h4>
<p>L‚Äôapproche la plus aboutie des <em>a priori</em> faiblement informatifs est peut-√™tre celle de Jeffreys. Ce dernier a propos√© une proc√©dure pour trouver une loi <em>a priori</em> avec une propri√©t√© d‚Äôinvariance par rapport √† la param√©trisation. Dans le cas univari√©, la loi <em>a priori</em> de Jeffreys est d√©fini par :
<span class="math display">\[\pi(\theta) \propto \sqrt{I(\theta)}\]</span>
o√π <span class="math inline">\(I\)</span> est la matrice d‚Äôinformation de Fisher (pour rappel, <span class="math inline">\(\textstyle I(\theta) = -\mathbb{E}_{Y|\theta}\left[\frac{\partial^2 \log (f(y|\theta))}{\partial\theta^2}\right]\)</span>). La loi <em>a priori</em> de Jeffreys est donc invariante pour les transformations bijectives des param√®tres. C‚Äôest-√†-dire que si nous consid√©rons une autre param√©trisation <span class="math inline">\(\gamma = g(\theta)\)</span> (pour laquelle il existe la bijection r√©ciproque <span class="math inline">\(g^{-1}(\cdot)\)</span>), on obtient toujours :
<span class="math display">\[\pi(\gamma) \propto \sqrt{I(\gamma)}\]</span>
tandis que <span class="math inline">\(\pi(\gamma)\)</span> correspond bien √† la m√™me loi <em>a priori</em> sur <span class="math inline">\(\theta\)</span>. Prenons ici des notations plus
rigoureuses, et notons les densit√©s <span class="math inline">\(\pi_\theta(\cdot)\)</span> et <span class="math inline">\(\pi_\gamma(\cdot)\)</span>. <span class="math inline">\(\pi_\gamma(\cdot)\)</span> s‚Äôexprime en fonction de
<span class="math inline">\(\pi_\theta(\cdot)\)</span> avec <span class="math inline">\(\pi_\gamma(\cdot) = \pi_\theta(\cdot)|J|\)</span>. On v√©rifie donc bien que <span class="math inline">\(\textstyle\sqrt{I(\gamma)} = \sqrt{I(\theta)}|J|\)</span>.</p>
<p>
<em>D√©monstration :</em> Soit <span class="math inline">\(F_X(x) = P(X &lt; x)\)</span> et consid√©rons la transformation non lin√©aire <span class="math inline">\(Y = g(X)\)</span>. On a alors <span class="math inline">\(F_Y(y) = P(Y &lt; y) = P(g(X) &lt; y) = P(X &lt; g^{-1}(y))\)</span>. La densit√© s‚Äôobtient naturellement en d√©rivant par rapport √† <span class="math inline">\(y\)</span> : <span class="math inline">\(\textstyle f_Y (y) = \frac{\partial g^{-1}(y)}{\partial y}f_X (g^{-1}(y))\)</span>. La formule s‚Äô√©tend au cas multidimensionnel o√π <span class="math inline">\(|J|\)</span> d√©signe le d√©terminant de la matrice jacobienne <span class="math inline">\(J\)</span> (matrice des d√©riv√©es partielles).</p>
<p>Dans le cas multidimensionnel (le plus courant) la loi <em>a priori</em> de Jeffreys est d√©finie comme :
<span class="math display">\[\pi(\theta) \propto \sqrt{ | I(\theta) | }\]</span>
o√π <span class="math inline">\(|I(\theta)|\)</span> est le d√©terminant de la matrice d‚Äôinformation de Fisher <span class="math inline">\(I(\theta)\)</span>. Cependant cette m√©thode est peu utilis√©e car d‚Äôune part les calculs peuvent √™tre compliqu√©s, et d‚Äôautre part elle peut donner des r√©sultats un peu curieux. En effet dans le cas d‚Äôune vraisemblance normale par exemple, o√π l‚Äôon a 2 param√®tres <span class="math inline">\(\theta\)</span> et <span class="math inline">\(\sigma\)</span>, l‚Äô<em>a priori</em> de Jeffreys multidimensionnel est <span class="math inline">\(1/\sigma^2\)</span>, ce qui est diff√©rent de <span class="math inline">\(\pi(\sigma) = 1/\sigma\)</span> obtenu dans le cas unidimensionnel‚Ä¶ Dans la pratique la tendance est d‚Äôappliquer la loi <em>a priori</em> de Jeffreys s√©par√©ment pour chaque param√®tre et de d√©finir la loi <em>a priori</em> conjointe par la multiplication des <em>a priori</em> pour chaque param√®tre (faisant donc une hypoth√®se d‚Äôind√©pendance). Pour l‚Äôexemple normal avec deux param√®tres, on obtient donc <span class="math inline">\(\pi(\theta,\sigma) = 1/\sigma\)</span>. Mais on note que ce n‚Äôest plus vraiment l‚Äô<em>a priori</em> de Jeffreys, en deux dimensions.</p>
<div class="Exercise">
<p><em>Exercice</em> : retrouver les r√©sultats √©nonc√©s ci-dessus (invariance pour la transformation <span class="math inline">\(\log\)</span> et r√©sultat pour la loi normale).</p>
</div>
<ul>
<li><strong>Loi <em>a priori</em> pour les familles √† param√®tres de position et l‚Äô√©chelle :</strong></li>
</ul>
<p>Consid√©rons les familles √† param√®tre de position, c‚Äôest-√†-dire dont les mod√®les d‚Äô√©chantillonnage sont de la forme <span class="math inline">\(f(y|\theta) = f(y - \theta)\)</span>. Des arguments d‚Äôinvariance permettent d‚Äôaffirmer que la loi non-informative pour <span class="math inline">\(\theta\)</span> devrait √™tre uniforme. Par les m√™mes arguments, on montre que pour les familles √† param√®tre d‚Äô√©chelle, c‚Äôest-√†-dire dont les mod√®les d‚Äô√©chantillonnage sont de la forme <span class="math inline">\(f(y|\sigma) = f(y/\sigma)\)</span>, la loi non-informative devrait √™tre <span class="math inline">\(\pi(\sigma) \propto 1/\sigma\)</span>. Plus g√©n√©ralement, pour les familles √† param√®tres de position et d‚Äô√©chelle, c‚Äôest-√†-dire dont les mod√®les d‚Äô√©chantillonnage sont de la forme <span class="math inline">\(f(y|\theta,\sigma) = f((y - \theta)/\sigma)\)</span>, l‚Äô<em>a priori</em> faiblement-informatif devrait √™tre de la forme <span class="math inline">\(\pi(\theta, \sigma) = 1/\sigma\)</span>. La loi normale est une famille de ce type, et pour elle cette recommandation d‚Äô<em>a priori</em> faiblement-informatif rejoint celle obtenue en multipliant les <em>a priori</em> de Jeffreys unidimensionnels, ainsi qu‚Äôindiqu√© plus haut.</p>
<div class="Exercise">
<p><em>Exercice</em> : retrouver les r√©sultats √©nonc√©s ci-dessus.</p>
</div>
</div>
<div id="lois-a-priori-diffuses" class="section level4">
<h4>Lois <em>a priori</em> diffuses</h4>
<p>En pratique, une alternative tr√®s courante pour donner une loi <em>a priori</em> faiblement informative est l‚Äôutilisation de lois param√©triques (telles que la loi normale) avec des param√®tres de variances tr√®s importants (ce qui se rapproche de la loi uniforme mais √©vite le probl√®me de loi impropre).</p>
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>(<a href="http://www.tonyohagan.co.uk/shelf/" class="uri">http://www.tonyohagan.co.uk/shelf/</a>).<a href="#fnref1" class="footnote-back">‚Ü©Ô∏é</a></p></li>
<li id="fn2"><p>(<a href="http://www.tonyohagan.co.uk/shelf/" class="uri">http://www.tonyohagan.co.uk/shelf/</a>).<a href="#fnref2" class="footnote-back">‚Ü©Ô∏é</a></p></li>
</ol>
</div>
