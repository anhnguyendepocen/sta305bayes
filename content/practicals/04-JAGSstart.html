---
title: "Exercice 4 : Prise en main de BUGS & JAGS"
linktitle: "Exercice 4 : BUGS & JAGS"
date: "2020-11-30"
exo_date: "2020-12-08"
menu:
  practicals:
    parent: "TP"
    weight: 4
type: docs
output:
  blogdown::html_page:
    toc: false
    number_sections: false
bibliography: "../../static/bib/references.bib"
---



<div class="Info">
<p>Le <a href="https://www.mrc-bsu.cam.ac.uk/software/bugs/">projet BUGS</a> (<em>Bayesian inference Using Gibbs Sampling</em>) a été initié en 1989 par l’unité de Biostatistique du MRC (<em>Medical Research Council</em>) de l’Université de Cambridge (au Royaume-Uni) afin de proposer un logiciel flexible pour l’analyse bayésienne de modèles statistiques complexes à l’aide d’algorithmes MCMC. Son implémentation la plus connue est <code>WinBUGS</code>, un logiciel clic-bouton disponible sous le système d’exploitation <em>Windows</em>. <code>OpenBUGS</code> est une implémentation fonctionnant sous <em>Windows</em>, <em>Mac OS</em> ou <em>Linux</em>. <a href="http://mcmc-jags.sourceforge.net/"><code>JAGS</code></a> (<em>Just another Gibbs Sampler</em>) est une autre implémentation plus récente qui s’appuie également sur le langage <code>BUGS</code>. Enfin, il faut également noter le logiciel <a href="http://mc-stan.org/"><code>STAN</code></a>, récemment développé à l’Université de Columbia qui n’est similaire à <code>BUGS</code> que dans son interface, s’appuyant sur des algorithmes MCMC innovants, comme par exemple les approches de Monte-Carlo Hamiltonien ou l’approche variationnelle. Une ressource très utile est le <a href="http://sourceforge.net/projects/mcmc-jags/files/Manuals/3.x/jags_user_manual.pdf">manuel de l’utilisateur de JAGS</a>.</p>
</div>
<p>Afin de se familiariser avec le logiciel <code>JAGS</code> (et son interface depuis <code>R</code> via le package <code>rjags</code>), nous nous intéressons ici à l’estimation a posteriori de la moyenne et de la variance d’un échantillon que l’on va modéliser avec une loi normale.</p>
<ol start="0" style="list-style-type: decimal">
<li><p>Charger le package <code>rjags</code>.</p>
<pre class="r"><code>library(rjags)</code></pre></li>
</ol>
<div class="Info">
<p>Un modèle <code>JAGS</code> a 3 éléments :</p>
<ul>
<li><em>le modèle</em> : spécifié dans un fichier externe (<code>.txt</code>) selon la syntaxe propre à <code>BUGS</code></li>
<li><em>les données</em> : une liste contenant chaque données sous un nom correspondant à celui utilisé dans le modèle</li>
<li><em>les valeurs initiales</em> : (optionnel) une liste contenant les valeurs initiales pour les différents paramètres à estimer</li>
</ul>
</div>
<ol style="list-style-type: decimal">
<li><p>Génerer <span class="math inline">\(N=50\)</span> observations selon une loi normale de moyenne <span class="math inline">\(m=2\)</span> et d’écart-type <span class="math inline">\(s=3\)</span>.</p>
<pre class="r"><code>N &lt;- 50  # le nombre d&#39;observations
obs &lt;- rnorm(n = N, mean = 2, sd = 3)  # les observations (simulée)</code></pre></li>
<li><p>Enregistrer dans un fichier <code>.txt</code> le modèle en <code>BUGS</code> défini à l’aide du code suivant :</p>
<pre class="bash"><code># Modèle
model{

  # Vraisemblance
  for (i in 1:N){ 
    obs[i]~dnorm(mu,tau)
  }

  # A priori
  mu~dnorm(0,0.0001) # propre mais très plat (faiblement informatif)
  tau~dgamma(0.0001,0.0001) # propre mais très plat (faiblement informatif)

  # Variables d&#39;interet
  sigma &lt;- pow(tau, -0.5)
}</code></pre>
<div class="Info">
<ul>
<li>Chaque fichier de spécification de modèle doit commencer par l’instruction <code>model{</code> qui indique à <code>JAGS</code> qu’il s’apprête à recevoir la spécification d’un modèle.</li>
<li>Ensuite il faut mettre en place le modèle en bouclant (boucle <em>for</em>) sur l’ensemble des données. Ici, nous voulons déclarer que qu’il y a <code>N</code> données, et que pour chacune d’entre elle <code>obs[i]</code> suit une loi normal (caractérisée avec la commande <code>dnorm</code>) de moyenne <code>mu</code> et de précision <code>tau</code>.<br />
⚠️ <em>Attention</em> : dans <code>BUGS</code> la distribution Normale est paramétrée par sa précision, qui est l’inverse de sa variance (<span class="math inline">\(\tau = 1/\sigma^2\)</span>).</li>
<li>Ensuite, il s’agit de définir les lois <em>a priori</em> pour nos deux paramètres <code>mu</code> et <code>tau</code>, qui sont identiques pour chaque données. Pour <code>mu</code>, nous prenons un <em>a priori</em> selon la loi Normale de moyenne <span class="math inline">\(0\)</span> et de précision <span class="math inline">\(10^{-4}\)</span> (donc de variance <span class="math inline">\(10\,000\)</span> : cela correspond à un <em>a priori</em> faiblement informatif et relativement étalé au vu de l’échelle de nos données. Pour <code>tau</code> nous prenons la loi <em>a priori</em> conjuquée dans un modèle Gaussien, c’est-à-dire la loi Gamma (avec des paramètres très faibles, là encore pour rester le moins informatif possible).</li>
<li>Enfin, nous donnons une définition déterministe de paramètre d’intérêt supplémentaire, ici l’écart-type <code>sigma</code>.<br />
<strong>NB</strong>: <code>~</code> indique une définition probabiliste d’une variable aléatoire, tandis que <code>&lt;-</code> indique une définition par une formule déterministe.</li>
</ul>
</div></li>
<li><p>À l’aide de la fonction <code>jags.model()</code>, créer un objet <code>jags</code> dans <code>R</code>.</p>
<pre class="r"><code>monpermierjags &lt;- jags.model(&quot;normalBUGSmodel.txt&quot;, data = list(&quot;obs&quot; = obs, 
                                                                &quot;N&quot; = length(obs)))</code></pre>
<pre><code>## Compiling model graph
##    Resolving undeclared variables
##    Allocating nodes
## Graph information:
##    Observed stochastic nodes: 50
##    Unobserved stochastic nodes: 2
##    Total graph size: 58
## 
## Initializing model</code></pre></li>
<li><p>À l’aide de la fonction <code>coda.samples()</code>, obtenir un échantillon de taille <span class="math inline">\(2\,000\)</span> des distributions <em>a posteriori</em> des paramètres de moyenne et d’écart-type.</p>
<pre class="r"><code>res &lt;- coda.samples(model = monpermierjags, variable.names = c(&quot;mu&quot;, &quot;sigma&quot;), n.iter = 2000)</code></pre></li>
<li><p>En étudiant le résultat renvoyé par la fonction <code>coda.samples</code>, calculer les estimateurs de la moyenne <em>a posteriori</em> et de la médiane <em>a posteriori</em> pour <code>mu</code> et <code>sigma</code> et donner un intervalles de crédibilité à 95% pour chacun.</p>
<pre class="r"><code>plot(res)</code></pre>
<p><img src="/practicals/04-JAGSstart_files/figure-html/codasamples-1.png" width="67%" /></p>
<pre class="r"><code>resum &lt;- summary(res)
resum</code></pre>
<pre><code>## 
## Iterations = 1:2000
## Thinning interval = 1 
## Number of chains = 1 
## Sample size per chain = 2000 
## 
## 1. Empirical mean and standard deviation for each variable,
##    plus standard error of the mean:
## 
##        Mean     SD Naive SE Time-series SE
## mu    1.417 0.4171 0.009328       0.009658
## sigma 2.964 0.3067 0.006858       0.006858
## 
## 2. Quantiles for each variable:
## 
##         2.5%   25%   50%   75% 97.5%
## mu    0.6112 1.138 1.413 1.701 2.236
## sigma 2.4371 2.744 2.941 3.156 3.624</code></pre>
<pre class="r"><code>resum$statistics[&quot;mu&quot;, &quot;Mean&quot;]</code></pre>
<pre><code>## [1] 1.417054</code></pre>
<pre class="r"><code>resum$statistics[&quot;sigma&quot;, &quot;Mean&quot;]</code></pre>
<pre><code>## [1] 2.963636</code></pre>
<pre class="r"><code>resum$quantiles[&quot;mu&quot;, &quot;50%&quot;]</code></pre>
<pre><code>## [1] 1.41316</code></pre>
<pre class="r"><code>resum$quantiles[&quot;sigma&quot;, &quot;50%&quot;]</code></pre>
<pre><code>## [1] 2.941281</code></pre>
<pre class="r"><code>resum$quantiles[&quot;mu&quot;, c(1, 5)]</code></pre>
<pre><code>##      2.5%     97.5% 
## 0.6111906 2.2362478</code></pre>
<pre class="r"><code>resum$quantiles[&quot;sigma&quot;, c(1, 5)]</code></pre>
<pre><code>##     2.5%    97.5% 
## 2.437123 3.624290</code></pre></li>
<li><p>Charger le package <code>coda</code>. Il s’agit d’un package contenant un certain nombre de fonctions pour le diagnostic de convergence des algorithmes MCMC et le traitement des résultats.</p>
<pre class="r"><code>library(coda)</code></pre></li>
<li><p>Afin de pouvoir diagnostiquer la convergence d’un algorithme MCMC, il est nécessaire de générer plusieurs chaînes de Markov différentes, partant de valeurs initiales différentes. Recréer un objet <code>jags</code> en spécifiant l’utilisation de 3 chaines de Markov parallèles et en initialisant <code>mu</code> et <code>tau</code> à <span class="math inline">\((0; -10; 100)\)</span> et <span class="math inline">\((1; 0,01; 0.1)\)</span> respectivement (<strong>ProTip:</strong> utiliser une <code>list</code> de <code>list</code> — une pour chaque chaine). Commenter.</p>
<pre class="r"><code>monjags2 &lt;- jags.model(&quot;normalBUGSmodel.txt&quot;, data = list(obs = obs, N = N), n.chains = 3, 
    inits = list(list(mu = 0, tau = 1), list(mu = -10, tau = 1/100), list(mu = 100, 
        tau = 1/10)))</code></pre>
<pre><code>## Compiling model graph
##    Resolving undeclared variables
##    Allocating nodes
## Graph information:
##    Observed stochastic nodes: 50
##    Unobserved stochastic nodes: 2
##    Total graph size: 58
## 
## Initializing model</code></pre>
<pre class="r"><code>res2 &lt;- coda.samples(model = monjags2, variable.names = c(&quot;mu&quot;, &quot;sigma&quot;), n.iter = 1000)
plot(res2)</code></pre>
<p><img src="/practicals/04-JAGSstart_files/figure-html/2e-jags-1.png" width="95%" /></p></li>
<li><p>À l’aide de la fonction <code>gelman.plot()</code>, tracer la statistique de Gelman-Rubin.</p>
<pre class="r"><code>gelman.plot(res2)</code></pre>
<p><img src="/practicals/04-JAGSstart_files/figure-html/gelman-1.png" width="95%" /></p></li>
<li><p>À l’aide des fonctions <code>autocorr.plot()</code> et <code>acfplot()</code> évaluer l’autocorrélation des paramètres étudiés.</p>
<pre class="r"><code>acfplot(res2)</code></pre>
<p><img src="/practicals/04-JAGSstart_files/figure-html/acf-1.png" width="672" height="20%" /></p>
<pre class="r"><code>par(mfrow = c(3, 2))
autocorr.plot(res2, ask = FALSE, auto.layout = FALSE)</code></pre>
<p><img src="/practicals/04-JAGSstart_files/figure-html/autocorr-1.png" width="672" /></p></li>
<li><p>À l’aide de la fonction <code>cumuplot()</code> évaluer les quantiles cumulés des paramètres étudiés. Comment les interpréter ?</p>
<pre class="r"><code>par(mfrow = c(3, 2))
cumuplot(res2, ask = FALSE, auto.layout = FALSE)</code></pre>
<p><img src="/practicals/04-JAGSstart_files/figure-html/cumuplot-1.png" width="672" /></p></li>
<li><p>À l’aide de la fonction <code>crosscorr.plot()</code> évaluer les corrélations entre les paramètres étudiés. Comment les interpréter ?</p>
<pre class="r"><code>crosscorr.plot(res2)</code></pre>
<p><img src="/practicals/04-JAGSstart_files/figure-html/cumplot-1.png" width="672" /></p></li>
<li><p>À l’aide de la fonction <code>hdi</code> du package <code>R</code> <code>HDInterval</code> donner les intervalles de crédibilités de plus haute densité <em>a posteriori</em> à 95%, et les comparer à ceux obtenu à partir des quantiles <span class="math inline">\(2,5\)</span>% et <span class="math inline">\(97,5\)</span>%.</p>
<pre class="r"><code>hdCI &lt;- HDInterval::hdi(res2)
hdCI</code></pre>
<pre><code>##              mu    sigma
## lower 0.5232886 2.431433
## upper 2.2128793 3.584501
## attr(,&quot;credMass&quot;)
## [1] 0.95</code></pre>
<pre class="r"><code>symCI &lt;- summary(res2)$quantiles[, c(1, 5)]
symCI</code></pre>
<pre><code>##            2.5%    97.5%
## mu    0.5608571 2.252754
## sigma 2.4477553 3.630992</code></pre>
<pre class="r"><code>symCI[, 2] - symCI[, 1]</code></pre>
<pre><code>##       mu    sigma 
## 1.691897 1.183237</code></pre>
<pre class="r"><code>hdCI[2, ] - hdCI[1, ]</code></pre>
<pre><code>##       mu    sigma 
## 1.689591 1.153068</code></pre></li>
</ol>
